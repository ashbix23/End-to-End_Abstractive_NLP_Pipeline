{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e779e07-e06f-464e-b142-35568baf5841",
   "metadata": {},
   "source": [
    "# 02 — Model Training\n",
    "\n",
    "This notebook fine-tunes the **BART-base** model on the *scientific_papers/arxiv* dataset.\n",
    "\n",
    "We'll:\n",
    "1. Load the tokenized dataset from cache.\n",
    "2. Load and configure the BART model.\n",
    "3. Fine-tune it using Hugging Face's `Seq2SeqTrainer`.\n",
    "4. Save the trained model checkpoint for evaluation.\n",
    "\n",
    "Dataset: full *scientific_papers/arxiv* (≈ 203k training samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbc2c44-72c9-491d-9aea-9205efec689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "\n",
    "from src.model import get_model, get_tokenizer\n",
    "from src.data_loader import load_or_build_tokenized\n",
    "from src.seed_utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c3e1d-2ff0-4fac-90f9-de3bb61ae2e2",
   "metadata": {},
   "source": [
    "## Load tokenized dataset\n",
    "\n",
    "We'll use the cached tokenized dataset created in the previous notebook.\n",
    "If not found, it will rebuild automatically.\n",
    "\n",
    "This dataset already includes:\n",
    "- `input_ids`\n",
    "- `attention_mask`\n",
    "- `labels` (with pad tokens replaced by -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd1c361-7543-4263-8f95-c048da75d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2c9b4b7c78416785727046f0dd433b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 203037, 'validation': 6436, 'test': 6440}\n",
      "Training samples: 40000\n",
      "Validation samples: 4000\n"
     ]
    }
   ],
   "source": [
    "raw, tok = load_or_build_tokenized(\n",
    "    dataset_name=\"scientific_papers\",\n",
    "    subset=\"arxiv\",\n",
    "    model_name=\"facebook/bart-base\",\n",
    "    max_input_len=1024,\n",
    "    max_target_len=256\n",
    ")\n",
    "\n",
    "print({split: len(tok[split]) for split in tok.keys()})\n",
    "train_subset = tok[\"train\"].select(range(40000))\n",
    "val_subset = tok[\"validation\"].select(range(4000))\n",
    "\n",
    "print(f\"Training samples: {len(train_subset)}\")\n",
    "print(f\"Validation samples: {len(val_subset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37db6ee-c85c-4a89-9682-50fc014196d2",
   "metadata": {},
   "source": [
    "## Load BART-base model and tokenizer\n",
    "\n",
    "We’ll initialize from `facebook/bart-base`.  \n",
    "Later you can experiment with `facebook/bart-large` or `t5-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a7ead5-6475-4653-9fc5-bf61a3ed4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "model = get_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0e7a5-96e6-4b15-aad9-6bf955897080",
   "metadata": {},
   "source": [
    "## Data collator\n",
    "\n",
    "`DataCollatorForSeq2Seq` dynamically pads sequences in each batch.  \n",
    "This keeps GPU/CPU memory usage efficient and ensures consistent shape for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2d456d-0732-4ea9-aa97-e432dd0a26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc7413-4f1b-4e3a-8644-a46f6651148e",
   "metadata": {},
   "source": [
    "## Define training arguments\n",
    "\n",
    "We use the full dataset but keep epochs and batch size modest for feasibility on macOS.\n",
    "The `Seq2SeqTrainer` will automatically:\n",
    "- Evaluate after each epoch.\n",
    "- Save checkpoints.\n",
    "- Restore the best model based on ROUGE-L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8383f30-2aec-4ade-b37f-471a72084171",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../outputs/model\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../outputs/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5115ea-46d7-4485-96d1-e9878fe57c82",
   "metadata": {},
   "source": [
    "## Initialize Trainer\n",
    "\n",
    "We’ll use the `Seq2SeqTrainer`, passing in:\n",
    "- The model\n",
    "- Tokenizer\n",
    "- Datasets\n",
    "- Data collator\n",
    "- Early stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd84a6df-bbb2-4887-ba9d-05ddf9548e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2294303/2309917333.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=val_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827d35b-cc66-49fe-af1d-a7128a9de885",
   "metadata": {},
   "source": [
    "## Start training\n",
    "\n",
    "The best checkpoint will be saved under `outputs/model/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d60f9e9-0eae-40ed-9490-716456e2584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 1:06:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.867600</td>\n",
       "      <td>2.502567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.475200</td>\n",
       "      <td>2.409363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20000, training_loss=2.67137275390625, metrics={'train_runtime': 3969.9789, 'train_samples_per_second': 20.151, 'train_steps_per_second': 5.038, 'total_flos': 4.829588153155584e+16, 'train_loss': 2.67137275390625, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17ad32-f0c0-4401-ad46-fe75d3371efe",
   "metadata": {},
   "source": [
    "## Save trained model\n",
    "\n",
    "Once training completes, we’ll save the final model and tokenizer to `outputs/model/`.  \n",
    "This directory will later be used by the evaluation and app scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72875340-afa3-4c3b-bb54-60097f57ba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../outputs/model/tokenizer_config.json',\n",
       " '../outputs/model/special_tokens_map.json',\n",
       " '../outputs/model/vocab.json',\n",
       " '../outputs/model/merges.txt',\n",
       " '../outputs/model/added_tokens.json',\n",
       " '../outputs/model/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../outputs/model\")\n",
    "tokenizer.save_pretrained(\"../outputs/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14721f7f-b2b3-4edd-bc4d-c3a5577767b7",
   "metadata": {},
   "source": [
    "## Training Summary\n",
    "\n",
    "- Model: `facebook/bart-base`\n",
    "- Dataset: `scientific_papers/arxiv`\n",
    "- Input length: 1024 tokens\n",
    "- Summary length: 256 tokens\n",
    "- Saved checkpoint: `outputs/model/`\n",
    "\n",
    "Next notebook: **03_model_evaluation.ipynb** — compute ROUGE & BERTScore and analyze examples.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
